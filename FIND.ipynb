{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   This section of the code simply sets up all possible variables we might want to change during training.\n",
    "\"\"\"\n",
    "import sys\n",
    "import random\n",
    "import numpy as np                     # Math and Deep Learning libraries\n",
    "import torch                \n",
    "from tqdm import tqdm                  # Pretty status bars\n",
    "from collections import defaultdict\n",
    "\n",
    "np.seterr(divide='ignore')             # Ignore divide by zero errors\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "# Use a GPU when possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load network architecture\n",
    "from utils.model import Net\n",
    "\n",
    "# Helper functions for running evaluation and visualization\n",
    "from utils.analysis import *           \n",
    "\n",
    "# Helper functions for converting strings to ints and sorting/padding data\n",
    "from utils.data_processing import *\n",
    "\n",
    "# Choose from configs.[bd, hco, nitro] or make your own\n",
    "from configs.bd import config\n",
    "config.device = device\n",
    "\n",
    "# TESTING\n",
    "# config.epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##    Infrastructure to process Data into Numpy Arrays of Integers\n",
    "##    We convert letters to numbers via the acids dictionary.\n",
    "##    We also compute the set of function labels (optionally training with binary)\n",
    "###############################################################################\n",
    "acids = {' ':0, 'A':1, 'C':2, 'E':3, 'D':4, 'G':5, 'F':6, 'I':7, 'H':8, \\\n",
    "         'K':9, 'M':10, 'L':11, 'N':12, 'Q':13, 'P':14, 'S':15, 'R':16, \\\n",
    "         'T':17, 'W':18, 'V':19, 'Y':20, 'X':21 }\n",
    "config.input_dim = len(acids)\n",
    "ints = {}\n",
    "for v in acids:\n",
    "    ints[acids[v]] = v\n",
    "\n",
    "L = [line.strip() for line in open(config.labels,'r')]\n",
    "\n",
    "# For logging we will store details of our training regime in the file name\n",
    "run_name = pretty(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##    If we are training a binary classifier, we need to resplit the data \n",
    "##    into chosen label vs OTHER\n",
    "###############################################################################\n",
    "lbls = {}    # Map label to integer\n",
    "ilbls = {}   # Map integer back to label\n",
    "\n",
    "# Perform Multiclass Classification\n",
    "if config.binary is None:\n",
    "    for v in L:\n",
    "        lbls[v] = len(lbls)\n",
    "        ilbls[lbls[v]] = v\n",
    "    config.num_labels = len(lbls)\n",
    "else:\n",
    "    # Split dataset into one-vs-all\n",
    "    for v in L:\n",
    "        if v == config.binary:\n",
    "            lbls[v] = 1\n",
    "            ilbls[lbls[v]] = v\n",
    "        else:\n",
    "            lbls[v] = 0\n",
    "            ilbls[lbls[v]] = \"OTHER\"\n",
    "    print(lbls, ilbls)\n",
    "    config.num_labels = 2\n",
    "    run_name += \"_\" + config.binary\n",
    "\n",
    "config.lbls = lbls\n",
    "config.ilbls = ilbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training counts\t\n",
      "E1: 373    C: 206    E2: 123    A: 47    E4: 41    B: 33    E3: 26\n",
      "Training    Inps:  849\n",
      "Training    Outs:  (849,)\n",
      "Validation  Inps:  117\n",
      "Validation  Outs:  (117,)\n",
      "Labels\t {'B': 0, 'E4': 1, 'E3': 2, 'E2': 3, 'C': 4, 'E1': 5, 'A': 6}\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "##    Data is loaded, Train/Validation/Test, counted, converted to numbers and \n",
    "##    stored in numpy arrays.\n",
    "###############################################################################\n",
    "\"\"\" Data & Parameters \"\"\"\n",
    "data = [line.strip().split() for line in open(config.training,'r')]\n",
    "val = [line.strip().split() for line in open(config.validation,'r')]\n",
    "\n",
    "# Sanity check formatting of the input data\n",
    "for vals in data:\n",
    "    if len(vals) != 2:\n",
    "        print(\"Problem: \" + vals)\n",
    "        sys.exit()\n",
    "\n",
    "strs, inputs, outputs = process(data, acids, lbls)\n",
    "\n",
    "print(\"Training counts\\t\")\n",
    "l_c = defaultdict(int) \n",
    "for v in outputs:\n",
    "    l_c[ilbls[v]] += 1\n",
    "V = [(l_c[v],v) for v in l_c]\n",
    "V.sort()\n",
    "V.reverse()\n",
    "print(\"    \".join([\"{}: {}\".format(lbl, cnt) for cnt,lbl in V]))\n",
    "\n",
    "count = np.zeros(len(ilbls), dtype=np.float32)\n",
    "\n",
    "for v in range(len(ilbls)):\n",
    "    if ilbls[v] in l_c:\n",
    "        count[v] += l_c[ilbls[v]]\n",
    "distr = np.sum(count)/(np.size(count)*count) #1. - count/np.sum(count)\n",
    "weight = torch.from_numpy(100*distr).to(device)\n",
    "\n",
    "inps, outs, strs = sort_data(inputs, outputs, strs)\n",
    "outs = np.array(outs)\n",
    "strs = np.array(strs)\n",
    "\n",
    "t_strs, t_inps, t_outs = process(val, acids, lbls)\n",
    "t_inps, t_outs, t_strs = sort_data(t_inps, t_outs, t_strs)\n",
    "t_outs = np.array(t_outs)\n",
    "t_strs = np.array(t_strs)\n",
    "\n",
    "print(\"Training    Inps: \", len(inputs))\n",
    "print(\"Training    Outs: \", outputs.shape)\n",
    "print(\"Validation  Inps: \", len(t_inps))\n",
    "print(\"Validation  Outs: \", t_outs.shape)\n",
    "print(\"Labels\\t\",lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.manual_seed(20180119)  <-- set a value for consistency\n",
    "net = Net(config=config)\n",
    "net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 27/27 [01:17<00:00,  2.87s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.07it/s]\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1         373   0.473 0.823 0.601   58    0.533 0.690 0.602\n",
      "C          206   0.562 0.306 0.396   24    0.429 0.750 0.545\n",
      "E2         123   0.089 0.041 0.056   17      nan 0.000   nan\n",
      "A          47      nan 0.000   nan   6       nan 0.000   nan\n",
      "E4         41      nan 0.000   nan   3       nan 0.000   nan\n",
      "B          33    0.000 0.000   nan   1       nan 0.000   nan\n",
      "E3         26      nan 0.000   nan   8       nan 0.000   nan\n",
      "Epoch: 0  Train Loss:  45.7304  Acc: 44.17  Val  Loss   5.8410  Acc: 49.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 27/27 [01:11<00:00,  2.66s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1         373   0.434 0.491 0.460   58    0.496 1.000 0.663\n",
      "C          206   0.293 0.607 0.395   24      nan 0.000   nan\n",
      "E2         123     nan 0.000   nan   17      nan 0.000   nan\n",
      "A          47      nan 0.000   nan   6       nan 0.000   nan\n",
      "E4         41      nan 0.000   nan   3       nan 0.000   nan\n",
      "B          33      nan 0.000   nan   1       nan 0.000   nan\n",
      "E3         26      nan 0.000   nan   8       nan 0.000   nan\n",
      "Epoch: 1  Train Loss:  41.3035  Acc: 36.28  Val  Loss   5.2298  Acc: 49.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "##    Training Loop\n",
    "###############################################################################\n",
    "prev_acc = 0\n",
    "for epoch in range(0, config.epochs + 1):\n",
    "\n",
    "    batches = []\n",
    "    indices = list(range(len(inps)))\n",
    "    for start in range(0, len(indices), config.batch_size):\n",
    "        batches.append((start, min(config.batch_size, len(indices)-start)))\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    train_acc = []\n",
    "  \n",
    "    net.reset_counts(epoch)\n",
    "    for start, b_size in tqdm(batches, ncols=80):\n",
    "        r = indices[start : start + b_size]\n",
    "        \n",
    "        # Setup\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.from_numpy(pad_data(inps[indices[start] : indices[start+b_size-1] + 1])).to(device)\n",
    "        labels = torch.from_numpy(outs[r]).to(device)\n",
    "\n",
    "        # Predict\n",
    "        net.train(mode=True)\n",
    "        logits, att, full = net(inputs)\n",
    "        ce_loss = net.loss(logits, labels, weight=weight)\n",
    "\n",
    "        # Compute loss and update\n",
    "        loss = ce_loss\n",
    "        total_loss += ce_loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Look at predictions\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        dists = full.permute(0,2,1).cpu().data.numpy()\n",
    "    \n",
    "        preds = preds.data.cpu().numpy()\n",
    "        np.add.at(net.pred_counts, preds, 1)\n",
    "        np.add.at(net.gold_counts, outs[r], 1)\n",
    "        np.add.at(net.corr_counts, preds[preds == outs[r]], 1)\n",
    "\n",
    "        train_acc.extend(list(preds == outs[r]))\n",
    "  \n",
    "    # Evaluate on validation (during training)\n",
    "    val_loss, val_acc = run_evaluation(net, t_inps, t_outs, t_strs, verbose=(epoch % 10 == 0 or epoch == config.epochs))\n",
    "\n",
    "    print(\"Epoch: {}  Train Loss: {:8.4f}  Acc: {:5.2f}  Val  Loss {:8.4f}  Acc: {:5.2f}\".format(epoch, \n",
    "          total_loss, 100*np.array(train_acc).mean(), val_loss, val_acc))\n",
    "\n",
    "    # Save best validation model for optimal generalization\n",
    "    if val_acc > prev_acc:\n",
    "        prev_acc = val_acc\n",
    "        pref = config.name\n",
    "        torch.save(net, \"{}.model\".format(run_name))\n",
    "\n",
    "out = torch.cat((net.embedding.weight.data, torch.ones(len(acids), 1).to(device)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example run of 100 epochs with training (left) and validation (right) precision, recall and F1 printed for every class.  Additionally, we print the overall  losses and accuracies after every epoch.  Note, these do not decrease monitonically but in general the model does perform better after some backtracking.  For most use cases a  shorter training regime is probably sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 27/27 [01:02<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1         373   0.536 0.936 0.682   \n",
      "C          206   0.793 0.762 0.777   \n",
      "E2         123     nan 0.000   nan   \n",
      "A          47      nan 0.000   nan   \n",
      "E4         41      nan 0.000   nan   \n",
      "B          33      nan 0.000   nan   \n",
      "E3         26      nan 0.000   nan   \n",
      "Acc: 59.600\n"
     ]
    }
   ],
   "source": [
    "config.load = \"1561246187_multi_BD_e1_h256_b32.model\"\n",
    "net = torch.load(config.load)                             # Load Saved Model\n",
    "net.to(device)\n",
    "\n",
    "# Run evaluation with best validation model on training\n",
    "val = [line.strip().split() for line in open(config.training,'r')]\n",
    "strs, inps, outs = process(val, acids, lbls)\n",
    "\n",
    "\n",
    "config.confusion = True\n",
    "config.pconfusion = False\n",
    "loss, acc = run_evaluation(net, inps, outs, strs, aggregate=True, verbose=True)\n",
    "print_predictors(net, \"final\")\n",
    "print(\"Acc: {:5.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix on training data\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data = [line.strip().replace(\"[\",\"\").replace(\"]\",\"\").split(\",\") for line in open(\"confusion.csv\")]\n",
    "labels = data[0][1:]\n",
    "data = [[float(v) for v in line[1:-1]] for line in data[1:]]\n",
    "df = pd.DataFrame(data, index = [i for i in labels], columns = [i for i in labels])\n",
    "df_norm_col = df.div(df.sum(axis=1), axis=0)\n",
    "plt.figure(figsize = (10,7))\n",
    "_ = sn.heatmap(df_norm_col, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with best validation model on validation\n",
    "val = [line.strip().split() for line in open(config.validation,'r')]\n",
    "_, inps, outs = process(val, acids, lbls)\n",
    "\n",
    "config.confusion = True\n",
    "config.pconfusion = False\n",
    "loss, acc = run_evaluation(net, inps, outs, showTrain=False, verbose=True)\n",
    "print(\"Acc: {:5.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with best validation model on test\n",
    "val = [line.strip().split() for line in open(config.testing,'r')]\n",
    "_, inps, outs = process(val, acids, lbls)\n",
    "\n",
    "loss, acc = run_evaluation(net, inps, outs, showTrain=False, verbose=True)\n",
    "print(\"Acc: {:5.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
